{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v08HXfHzKwdO"
   },
   "source": [
    "# References\n",
    "\n",
    "This code is based on the work by DiSipio, Huang, Chen described in the paper in reference 1 below.\n",
    "\n",
    "1. Di Sipio R, Huang J-H, Chen SY-C, et al (2022) The Dawn of Quantum Natural Language Processing. In: ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, Singapore, Singapore, pp 8612–8616\n",
    "2. Chen SY-C, Yoo S, Fang Y-LL (2022) Quantum Long Short-Term Memory. In: ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, Singapore, Singapore, pp 8622–8626\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import pyplot\n",
    "\n",
    "import pennylane as qml\n",
    "from pennylane import numpy as np\n",
    "from pennylane.templates.embeddings import AngleEmbedding, AmplitudeEmbedding\n",
    "from pennylane.optimize import AdamOptimizer\n",
    "from pennylane.optimize import NesterovMomentumOptimizer\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.metrics import AUC\n",
    "\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aua0ZLfGOHda",
    "outputId": "9203472f-abaf-4c33-c353-4fcd654f2ac0"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/MyDrive')\n",
    "SystemCallData=\"./MyData/MySystemCallList.csv\"\n",
    "SysData = pd.read_csv(SystemCallData)\n",
    "SysData = SysData[[\"SystemCallName\"]] \n",
    "SysDataList=SysData['SystemCallName'].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "X0H_ifdZ6ROz"
   },
   "outputs": [],
   "source": [
    "word_to_ix = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "NmNrHhUw53SD"
   },
   "outputs": [],
   "source": [
    "for systemcall in SysData['SystemCallName']:\n",
    "  if systemcall not in word_to_ix:  # word has not been assigned an index yet\n",
    "      word_to_ix[systemcall] = len(word_to_ix)  # Assign each word with a unique index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "FviX-NoG53Xi"
   },
   "outputs": [],
   "source": [
    "def split_sequence(sequence, n_steps):\n",
    "\tX, y = list(), list()\n",
    "\tfor i in range(len(sequence)):\n",
    "\t\t# find the end of this pattern\n",
    "\t\tend_ix = i + n_steps\n",
    "\t\t# check if we are beyond the sequence\n",
    "\t\tif end_ix > len(sequence)-1:\n",
    "\t\t\tbreak\n",
    "\t\t# gather input and output parts of the pattern\n",
    "\t\tseq_x, seq_y = sequence[i:end_ix], sequence[i+1:end_ix+1]\n",
    "\t\tX.append(seq_x)\n",
    "\t\ty.append(seq_y)\n",
    "\treturn array(X), array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "NLB43QEzIt9z"
   },
   "outputs": [],
   "source": [
    "# choose a number of time steps\n",
    "from numpy import array\n",
    "n_steps = 15 # 3, 5, 10, 15, 20\n",
    "\n",
    "n_steps_list = [3,5,10,15,20]\n",
    "# split into samples\n",
    "h=SysDataList\n",
    "#h=[1,2,3,4,5,6,7,8,9,10]\n",
    "X, y = split_sequence(h, n_steps)\n",
    "\n",
    "#trainingdata_list = [split_sequence(h, i) for i in n_steps_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "VvvP920hknzp"
   },
   "outputs": [],
   "source": [
    "trainingdata_list = (split_sequence(h, i) for i in n_steps_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "iNjc51AsK4Aj"
   },
   "outputs": [],
   "source": [
    "trainingdata=[X,y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "l3ooYG5AKwdU"
   },
   "outputs": [],
   "source": [
    "def prepare_sequence(seq, to_ix):\n",
    "    idxs = [to_ix[w] for w in seq]\n",
    "    return torch.tensor(idxs, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLSTM(nn.Module):\n",
    "    def __init__(self, \n",
    "                input_size, \n",
    "                hidden_size, \n",
    "                n_qubits=4,\n",
    "                n_qlayers=1,\n",
    "                batch_first=True,\n",
    "                return_sequences=False, \n",
    "                return_state=False,\n",
    "                backend=\"default.qubit\"):\n",
    "        super(QLSTM, self).__init__()\n",
    "        self.n_inputs = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.concat_size = self.n_inputs + self.hidden_size\n",
    "        self.n_qubits = n_qubits\n",
    "        self.n_qlayers = n_qlayers\n",
    "        self.backend = backend  # \"default.qubit\", \"qiskit.basicaer\", \"qiskit.ibm\"\n",
    "\n",
    "        self.batch_first = batch_first\n",
    "        self.return_sequences = return_sequences\n",
    "        self.return_state = return_state\n",
    "\n",
    "        self.wires_forget = [f\"wire_forget_{i}\" for i in range(self.n_qubits)]\n",
    "        self.wires_input = [f\"wire_input_{i}\" for i in range(self.n_qubits)]\n",
    "        self.wires_update = [f\"wire_update_{i}\" for i in range(self.n_qubits)]\n",
    "        self.wires_output = [f\"wire_output_{i}\" for i in range(self.n_qubits)]\n",
    "\n",
    "        self.dev_forget = qml.device(self.backend, wires=self.wires_forget)\n",
    "        self.dev_input = qml.device(self.backend, wires=self.wires_input)\n",
    "        self.dev_update = qml.device(self.backend, wires=self.wires_update)\n",
    "        self.dev_output = qml.device(self.backend, wires=self.wires_output)\n",
    "\n",
    "        def _circuit_forget(inputs, weights):\n",
    "            qml.templates.AngleEmbedding(inputs, wires=self.wires_forget)\n",
    "            qml.templates.BasicEntanglerLayers(weights, wires=self.wires_forget)\n",
    "            return [qml.expval(qml.PauliZ(wires=w)) for w in self.wires_forget]\n",
    "        self.qlayer_forget = qml.QNode(_circuit_forget, self.dev_forget, interface=\"torch\")\n",
    "\n",
    "        def _circuit_input(inputs, weights):\n",
    "            qml.templates.AngleEmbedding(inputs, wires=self.wires_input)\n",
    "            qml.templates.BasicEntanglerLayers(weights, wires=self.wires_input)\n",
    "            return [qml.expval(qml.PauliZ(wires=w)) for w in self.wires_input]\n",
    "        self.qlayer_input = qml.QNode(_circuit_input, self.dev_input, interface=\"torch\")\n",
    "\n",
    "        def _circuit_update(inputs, weights):\n",
    "            qml.templates.AngleEmbedding(inputs, wires=self.wires_update)\n",
    "            qml.templates.BasicEntanglerLayers(weights, wires=self.wires_update)\n",
    "            return [qml.expval(qml.PauliZ(wires=w)) for w in self.wires_update]\n",
    "        self.qlayer_update = qml.QNode(_circuit_update, self.dev_update, interface=\"torch\")\n",
    "\n",
    "        def _circuit_output(inputs, weights):\n",
    "            qml.templates.AngleEmbedding(inputs, wires=self.wires_output)\n",
    "            qml.templates.BasicEntanglerLayers(weights, wires=self.wires_output)\n",
    "            return [qml.expval(qml.PauliZ(wires=w)) for w in self.wires_output]\n",
    "        self.qlayer_output = qml.QNode(_circuit_output, self.dev_output, interface=\"torch\")\n",
    "\n",
    "        weight_shapes = {\"weights\": (n_qlayers, n_qubits)}\n",
    "        print(f\"weight_shapes = (n_qlayers, n_qubits) = ({n_qlayers}, {n_qubits})\")\n",
    "\n",
    "        self.clayer_in = torch.nn.Linear(self.concat_size, n_qubits)\n",
    "        self.VQC = {\n",
    "            'forget': qml.qnn.TorchLayer(self.qlayer_forget, weight_shapes),\n",
    "            'input': qml.qnn.TorchLayer(self.qlayer_input, weight_shapes),\n",
    "            'update': qml.qnn.TorchLayer(self.qlayer_update, weight_shapes),\n",
    "            'output': qml.qnn.TorchLayer(self.qlayer_output, weight_shapes)\n",
    "        }\n",
    "        self.clayer_out = torch.nn.Linear(self.n_qubits, self.hidden_size)\n",
    "\n",
    "    def draw_mpl(self, style, inp, weights):\n",
    "        qml.drawer.use_style(style)\n",
    "        fig, ax = qml.draw_mpl(self.qlayer_forget)(inp, weights)\n",
    "        fig, ax = qml.draw_mpl(self.qlayer_input)(inp, weights)\n",
    "        fig, ax = qml.draw_mpl(self.qlayer_update)(inp, weights)\n",
    "        fig, ax = qml.draw_mpl(self.qlayer_output)(inp, weights)\n",
    "        plt.show()\n",
    "\n",
    "    def snapshots(self, inp, weights):\n",
    "        print(\"forget: \", qml.snapshots(self.qlayer_forget)(inp, weights))\n",
    "        print(\"input: \", qml.snapshots(self.qlayer_input)(inp, weights))\n",
    "        print(\"update: \", qml.snapshots(self.qlayer_update)(inp, weights))\n",
    "        print(\"output: \", qml.snapshots(self.qlayer_output)(inp, weights))\n",
    "\n",
    "    def print(self, inp, weights):\n",
    "        print(qml.draw(self.qlayer_forget, expansion_strategy=\"device\")(inp, weights))\n",
    "        print(qml.draw(self.qlayer_input, expansion_strategy=\"device\")(inp, weights))\n",
    "        print(qml.draw(self.qlayer_update, expansion_strategy=\"device\")(inp, weights))\n",
    "        print(qml.draw(self.qlayer_output, expansion_strategy=\"device\")(inp, weights))\n",
    "\n",
    "    def forward(self, x, init_states=None):\n",
    "        '''\n",
    "        x.shape is (batch_size, seq_length, feature_size)\n",
    "        recurrent_activation -> sigmoid\n",
    "        activation -> tanh\n",
    "        '''\n",
    "        if self.batch_first is True:\n",
    "            batch_size, seq_length, features_size = x.size()\n",
    "        else:\n",
    "            seq_length, batch_size, features_size = x.size()\n",
    "\n",
    "        hidden_seq = []\n",
    "        if init_states is None:\n",
    "            h_t = torch.zeros(batch_size, self.hidden_size)  # hidden state (output)\n",
    "            c_t = torch.zeros(batch_size, self.hidden_size)  # cell state\n",
    "        else:\n",
    "            h_t, c_t = init_states\n",
    "            h_t = h_t[0]\n",
    "            c_t = c_t[0]\n",
    "\n",
    "        for t in range(seq_length):\n",
    "            # get features from the t-th element in seq, for all entries in the batch\n",
    "            x_t = x[:, t, :]\n",
    "            \n",
    "            # Concatenate input and hidden state\n",
    "            v_t = torch.cat((h_t, x_t), dim=1)\n",
    "\n",
    "            # match qubit dimension\n",
    "            y_t = self.clayer_in(v_t)\n",
    "\n",
    "            f_t = torch.sigmoid(self.clayer_out(self.VQC['forget'](y_t)))  # forget block\n",
    "            i_t = torch.sigmoid(self.clayer_out(self.VQC['input'](y_t)))  # input block\n",
    "            g_t = torch.tanh(self.clayer_out(self.VQC['update'](y_t)))  # update block\n",
    "            o_t = torch.sigmoid(self.clayer_out(self.VQC['output'](y_t))) # output block\n",
    "\n",
    "            c_t = (f_t * c_t) + (i_t * g_t)\n",
    "            h_t = o_t * torch.tanh(c_t)\n",
    "\n",
    "            hidden_seq.append(h_t.unsqueeze(0))\n",
    "        hidden_seq = torch.cat(hidden_seq, dim=0)\n",
    "        hidden_seq = hidden_seq.transpose(0, 1).contiguous()\n",
    "        return hidden_seq, (h_t, c_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "UcXu6uUpKwdW"
   },
   "outputs": [],
   "source": [
    "class LSTMTagger(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size, n_qubits=0):\n",
    "        super(LSTMTagger, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_qubits = n_qubits\n",
    "\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim.\n",
    "        if n_qubits > 0:\n",
    "            print(\"Quantum LSTM\")\n",
    "            self.lstm = QLSTM(embedding_dim, hidden_dim, n_qubits=n_qubits)\n",
    "        else:\n",
    "            print(\"Classical LSTM\")\n",
    "            self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "\n",
    "        # The linear layer that maps from hidden state space to tag space\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        lstm_out, _ = self.lstm(embeds.view(len(sentence), 1, -1))\n",
    "        tag_logits = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
    "        tag_scores = F.log_softmax(tag_logits, dim=1)\n",
    "        return tag_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "3nDYN_Z8KwdX"
   },
   "outputs": [],
   "source": [
    "embedding_dim = 8\n",
    "hidden_dim = 6    # 2,4,6,8,10,12,14\n",
    "hidden_dim_list = list(range(2,16,2))\n",
    "n_epochs = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GDMR60Ka29YN",
    "outputId": "ef85f818-b0a0-45f7-d73b-d529813ed14d"
   },
   "outputs": [],
   "source": [
    "models_classical = []\n",
    "\n",
    "for i in hidden_dim_list:\n",
    "\n",
    "  models_classical.append(LSTMTagger(embedding_dim, \n",
    "                          i, \n",
    "                          vocab_size=len(word_to_ix), \n",
    "                          tagset_size=len(word_to_ix), \n",
    "                          n_qubits=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "3dqd01pfdtYJ"
   },
   "outputs": [],
   "source": [
    "def train(model, n_epochs, trainingdata=trainingdata, name=\"\", file=\"\"):\n",
    "    loss_function = nn.NLLLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "    #first = True\n",
    "    history = {\n",
    "        'loss': [],\n",
    "        'acc': []\n",
    "    }\n",
    "    for epoch in range(n_epochs):\n",
    "        losses = []\n",
    "        preds = []\n",
    "        targets = []\n",
    "        for i in range(len(trainingdata)):\n",
    "            # Step 1. Remember that Pytorch accumulates gradients.\n",
    "            # We need to clear them out before each instance\n",
    "            model.zero_grad()\n",
    "\n",
    "            # Step 2. Get our inputs ready for the network, that is, turn them into\n",
    "            # Tensors of word indices.\n",
    "            sentence_in = prepare_sequence(trainingdata[0][i], word_to_ix)\n",
    "            labels = prepare_sequence(trainingdata[1][i], word_to_ix)\n",
    "\n",
    "            # Step 3. Run our forward pass.\n",
    "            tag_scores = model(sentence_in)\n",
    "\n",
    "            # Step 4. Compute the loss, gradients, and update the parameters by\n",
    "            #  calling optimizer.step()\n",
    "            loss = loss_function(tag_scores, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            losses.append(float(loss))\n",
    "            \n",
    "            probs = torch.softmax(tag_scores, dim=-1)\n",
    "            preds.append(probs.argmax(dim=-1))\n",
    "            targets.append(labels)\n",
    "\n",
    "        avg_loss = np.mean(losses)\n",
    "        history['loss'].append(avg_loss)\n",
    "        \n",
    "        preds = torch.cat(preds)\n",
    "        targets = torch.cat(targets)\n",
    "        corrects = (preds == targets)\n",
    "        accuracy = corrects.sum().float() / float(targets.size(0) )\n",
    "        history['acc'].append(accuracy)\n",
    "\n",
    "        string = f\"Epoch {epoch+1} / {n_epochs}: Loss = {avg_loss:.3f} Acc = {accuracy:.2f}\"\n",
    "\n",
    "        string = (name + \" \" + string) if name else string\n",
    "        print(string)\n",
    "\n",
    "        if file:\n",
    "          \n",
    "          with open(file, 'a', newline='') as f:\n",
    "            # if first:\n",
    "            #   temp = max(accuracy)\n",
    "            #   f.write(f'Max accuracy = {temp:.2f} at Epoch = {accuracy.index(temp)},\\n')\n",
    "            #   first = False\n",
    "            f.write(string+',\\n')\n",
    "            # writer = csv.writer(f)\n",
    "            # writer.writerow(string)\n",
    "    # if file:\n",
    "    #       with open(file, 'a', newline='') as f:\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "59kX6ibiIdja"
   },
   "outputs": [],
   "source": [
    "#train(models_classical[0], n_epochs, data,name=f\"Classical hidden_dim={models_classical[0].hidden_dim}_n_steps={n_steps_list[0]}\", file=\"/QUANTUM/save.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "05OIigxYeI-I"
   },
   "outputs": [],
   "source": [
    "#history_classical = train(model_classical, n_epochs)\n",
    "\n",
    "# his_model = train(models_classical[0], n_epochs, list(next(trainingdata_list)), \n",
    "#                                      name=f\"Classical hidden_dim={hidden_dim_list[0]}:n_steps={n_steps_list[0]}\",\n",
    "#                                      file=\"/content/epochs.csv\")\n",
    "\n",
    "# histories_classical = []\n",
    "\n",
    "# for k,i in enumerate(models_classical):\n",
    "#   count = 0\n",
    "#   for train_data in trainingdata_list:\n",
    "#     histories_classical.append([train(i, n_epochs, list(train_data), \n",
    "#                                      name=f\"Classical hidden_dim={hidden_dim_list[k]}:n_steps={n_steps_list[count]}\",\n",
    "#                                      file=\"/content/epochs.csv\"), \n",
    "#                                 hidden_dim_list[k]])\n",
    "#     count += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xa2sXAs7KwdY"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P404gVcZKwda",
    "outputId": "9af22c63-bc52-4c26-f93c-2f43da76202a"
   },
   "outputs": [],
   "source": [
    "n_qubits = 1 # 2,3,4,5,6,7,8,9,10\n",
    "n_qubits_list = list(range(1,11))\n",
    "\n",
    "model_quantum = LSTMTagger(embedding_dim, \n",
    "                        hidden_dim, \n",
    "                        vocab_size=len(word_to_ix), \n",
    "                        tagset_size=len(word_to_ix), \n",
    "                        n_qubits=n_qubits)\n",
    "\n",
    "models_quantum = []\n",
    "\n",
    "for i in hidden_dim_list:\n",
    "\n",
    "  for j in n_qubits_list:\n",
    "\n",
    "    models_quantum.append(LSTMTagger(embedding_dim, \n",
    "                            i, \n",
    "                            vocab_size=len(word_to_ix), \n",
    "                            tagset_size=len(word_to_ix), \n",
    "                            n_qubits=j))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VKsHjtriKwda"
   },
   "source": [
    "### Plot the training history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "YlXDtn10Kwdb"
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def plot_history(history_classical, history_quantum, hidden_dim_classical=0, hidden_dim_quantum=0, n_qubits=0, file=\"pos_training\"):\n",
    "    loss_c = history_classical['loss']\n",
    "    acc_c = history_classical['acc']\n",
    "    loss_q = history_quantum['loss']\n",
    "    acc_q = history_quantum['acc']\n",
    "    n_epochs = max([len(loss_c), len(loss_q)])\n",
    "    x_epochs = [i for i in range(n_epochs)]\n",
    "    \n",
    "    fig, ax1 = plt.subplots()\n",
    "    \n",
    "    ax1.set_xlabel(\"Epoch\")\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "    \n",
    "    ax1.plot(loss_c, label=\"Classical LSTM loss\", color='orange', linestyle='dashed')\n",
    "    ax1.plot(loss_q, label=\"Quantum LSTM loss\", color='red', linestyle='solid')\n",
    "\n",
    "    ax2 = ax1.twinx()\n",
    "    \n",
    "    #ax2.set_ylabel(\"Accuracy\")\n",
    "    ax2.plot(acc_c, label=\"Classical LSTM accuracy\", color='steelblue', linestyle='dashed')\n",
    "    ax2.plot(acc_q, label=\"Quantum LSTM accuracy\", color='blue', linestyle='solid')\n",
    "    ax1.tick_params(right=False)  \n",
    "    ax2.tick_params(right=False)\n",
    "    plt.title(f\"hidden_dim_c={hidden_dim_classical}    n_qubits={n_qubits}   hidden_dim_q={hidden_dim_quantum}\")\n",
    "    plt.ylim(0., 1.1)\n",
    "    #plt.legend(loc=\"upper right\")\n",
    "    ax2.yaxis.set_visible(False)\n",
    "    fig.legend(loc=\"upper right\", bbox_to_anchor=(1.55,1), bbox_transform=ax1.transAxes)\n",
    "\n",
    "    #plt.savefig(file+\".pdf\", bbox_inches='tight')\n",
    "    plt.savefig(file+\".png\", bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "NNHFKsnZ4p9t"
   },
   "outputs": [],
   "source": [
    "#data = next(trainingdata_list)\n",
    "data = split_sequence(h, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "v_gVCNIbKwda",
    "outputId": "52951e77-71ea-4fa2-b5cd-abd814eb3fb1"
   },
   "outputs": [],
   "source": [
    "# history_quantum = train(model_quantum, n_epochs)\n",
    "\n",
    "# histories_quantum = []\n",
    "\n",
    "count = 6\n",
    "count_steps = 0\n",
    "\n",
    "his_c_model = train(models_classical[count], n_epochs, data, \n",
    "                    name=f\"Classical hidden_dim={models_classical[count].hidden_dim}_n_steps={n_steps_list[count_steps]}\",\n",
    "                    file=f\"/QUANTUM/classical_hidden_dim_{models_classical[count].hidden_dim}_n_steps={n_steps_list[count_steps]}_epochs.csv\")\n",
    "\n",
    "for i in models_quantum:\n",
    "\n",
    "  his_q_model = train(i, n_epochs, data,\n",
    "                      name=f\"Quantum hidden_dim={i.hidden_dim}_n_steps={n_steps_list[count_steps]}_n_qubits={i.n_qubits}\",\n",
    "                      file=f\"/QUANTUM/quantum_hidden_dim_{i.hidden_dim}_n_qubits_{i.n_qubits}_n_steps={n_steps_list[count_steps]}_epochs.csv\")\n",
    "  \n",
    "  plot_history(his_c_model, his_q_model, models_classical[count].hidden_dim, i.hidden_dim, i.n_qubits, f\"/PHOTOS/hidden_dim_c_{models_classical[count].hidden_dim}_n_qubits_{i.n_qubits}_hidden_dim_q_{i.hidden_dim}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r_z-Xdjrdfnd",
    "outputId": "60b270ab-6d63-4a41-9050-ea3b3b7ed15e"
   },
   "outputs": [],
   "source": [
    "!zip -r /QUANTUM.zip /QUANTUM/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gy0kZT0ygF_s",
    "outputId": "d2cc66a3-8d6b-44ab-e336-a5f58ed3c2a0"
   },
   "outputs": [],
   "source": [
    "!zip -r /PHOTOS.zip /PHOTOS/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "TgZJpg5k8dmt",
    "outputId": "940067c9-d772-4414-e0f4-6e68c26a7f05"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "files.download('/QUANTUM.zip')\n",
    "files.download('/PHOTOS.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "cRTxsnKfK6kx",
    "outputId": "c5557180-da4c-4ddc-d350-091e5dd483c9"
   },
   "outputs": [],
   "source": [
    "plot_history(his_model, his_q_model, hidden_dim_list[0], hidden_dim_list[0], 1, f\"/hidden_dim_c_{hidden_dim_list[0]}_n_qubits_{1}_hidden_dim_quantum_{hidden_dim_list[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dnxSqePCvxdO"
   },
   "outputs": [],
   "source": [
    "for history, hidden_dim in histories_classical:\n",
    "  for q_history, q_hidden_dim, n_qubits in histories_quantum:\n",
    "    plot_history(history, q_history, hidden_dim, q_hidden_dim, n_qubits, f\"/content/drive/quantum_photos/hidden_dim_c_{hidden_dim}_n_qubits_{n_qubits}_hidden_dim_quantum_{q_hidden_dim}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 169
    },
    "id": "SKiOvSthKwdb",
    "outputId": "b28264a4-740a-4116-e7cc-8413ae08bd34"
   },
   "outputs": [],
   "source": [
    "plot_history(history_classical, history_quantum, 3, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tvp1fKJXKwdb"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
